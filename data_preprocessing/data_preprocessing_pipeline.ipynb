{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nrows_to_load = 10000000\n",
    "\n",
    "unique_vals = {\n",
    "    'category_1': ['N', 'Y', 'Z'],\n",
    "    'category_3': ['A', 'B', 'C', 'Z'],\n",
    "    'category_2': [1.0, -1, 3.0, 5.0, 2.0, 4.0],\n",
    "    'state_id': [16,9,-1,11,15,17,5,24,19,23,3,8,18,7,4,22,13,1,10,21,20,14,2,12,6],\n",
    "    'category_4': ['N', 'Y', 'Z'],\n",
    "    'most_recent_purchases_range': ['E', 'D', 'C', 'B', 'A', 'Z'],\n",
    "    'most_recent_sales_range': ['E', 'D', 'C', 'B', 'A', 'Z']\n",
    "}\n",
    "\n",
    "# Load in all historical transactions.\n",
    "hist_df = pd.concat([\n",
    "    pd.read_csv(\n",
    "        \"../input/historical_transactions.csv\", \n",
    "        index_col='merchant_id'\n",
    "    ),\n",
    "    pd.read_csv(\n",
    "        \"../input/new_merchant_transactions.csv\", \n",
    "        index_col='merchant_id'\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in merchant information.\n",
    "merchants_df = (\n",
    "    pd.read_csv(\"../input/merchants.csv\")\n",
    "    .drop_duplicates(subset='merchant_id', keep='first')\n",
    "    .set_index('merchant_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(hist_df, merchants_df, i, final_set=False):\n",
    "    if final_set:\n",
    "        hist_df_batch = hist_df.iloc[i*nrows_to_load:, :]\n",
    "    else:\n",
    "        hist_df_batch = hist_df.iloc[i*nrows_to_load:(i+1)*nrows_to_load, :]\n",
    "    print(\"Processing batch: \", i)\n",
    "    print(\"Batch size: \", hist_df_batch.shape)\n",
    "\n",
    "    cols_to_use = merchants_df.columns.difference(hist_df_batch.columns)\n",
    "\n",
    "    # Join historical transactions with merchant information where \n",
    "    # transactions were made and drop uneccessary columns right after that.\n",
    "    hist_transactions = (\n",
    "        hist_df_batch\n",
    "        .join(\n",
    "            merchants_df[cols_to_use],\n",
    "            how='left',\n",
    "            on='merchant_id',\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(columns=[\n",
    "            'avg_sales_lag12', 'avg_sales_lag6', 'avg_sales_lag3', \n",
    "            'active_months_lag3', 'active_months_lag6', \n",
    "            'avg_purchases_lag3', 'avg_purchases_lag6', \n",
    "            'merchant_group_id'\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Create a column for number of days between purchases using a window function.\n",
    "    hist_transactions['purchase_date'] = pd.to_datetime(hist_transactions['purchase_date'])\n",
    "    hist_transactions['purchase_date_day'] = pd.to_datetime(hist_transactions['purchase_date']).dt.day\n",
    "    hist_transactions['days_between_purch'] = hist_transactions['purchase_date_day'].rolling(2).sum()\n",
    "\n",
    "    # Some info is missing because not all transactions have a \n",
    "    # merchant matched with them. We don't want to lose transactions because \n",
    "    # then we might not have any data for some card_ids, so will fill \n",
    "    # out missing rows with special values (-1 for numerical and 'Z' for categorical).\n",
    "    hist_transactions['days_between_purch'] = hist_transactions['days_between_purch'].fillna(30) # 30 - is the average value.\n",
    "    hist_transactions['category_2'] = hist_transactions['category_2'].fillna(-1)\n",
    "    hist_transactions['category_3'] = hist_transactions['category_3'].fillna('Z')\n",
    "    hist_transactions['category_4'] = hist_transactions['category_4'].fillna('Z')\n",
    "    hist_transactions['active_months_lag12'] = hist_transactions['active_months_lag12'].fillna(-1)\n",
    "    hist_transactions['avg_purchases_lag12'] = hist_transactions['avg_purchases_lag12'].fillna(-1)\n",
    "    hist_transactions['most_recent_purchases_range'] = hist_transactions['most_recent_purchases_range'].fillna('Z')\n",
    "    hist_transactions['most_recent_sales_range'] = hist_transactions['most_recent_sales_range'].fillna('Z')\n",
    "    hist_transactions['numerical_1'] = hist_transactions['numerical_1'].fillna(hist_transactions['numerical_1'].mean())\n",
    "    hist_transactions['numerical_2'] = hist_transactions['numerical_2'].fillna(hist_transactions['numerical_2'].mean())\n",
    "    hist_transactions['authorized_flag_binary'] = hist_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "\n",
    "    categorical_cols = [\n",
    "        'category_1', 'category_3', 'category_2', 'category_4','state_id',\n",
    "        'most_recent_purchases_range', 'most_recent_sales_range'\n",
    "    ]\n",
    "    \n",
    "    # Since we are processing data in batches, some batches might not span all\n",
    "    # of the possible categories within categorical columns, so we are going\n",
    "    # to preset all categorical columns with all possible categories that \n",
    "    # they can take on, even if some categories are missing in the batch of \n",
    "    # data that is currently being processed.\n",
    "    for cat in categorical_cols:\n",
    "        hist_transactions[cat] = hist_transactions[cat].astype('category', categories=unique_vals[cat])\n",
    "\n",
    "    to_process_cols = [\n",
    "        'category_1', 'category_2', 'category_3', 'category_4', 'state_id', \n",
    "        'most_recent_purchases_range', 'most_recent_sales_range',\n",
    "    ]\n",
    "    vect_category_cols = []\n",
    "\n",
    "    # Convert all categorical columns into dummy variables.\n",
    "    for cat in to_process_cols:\n",
    "        vect_category_cols += [cat + '_' + str(col) for col in list(hist_transactions[cat].unique())]\n",
    "        dummies = pd.get_dummies(hist_transactions[cat], prefix=cat)\n",
    "        hist_transactions = pd.concat([hist_transactions, dummies], axis=1)\n",
    "\n",
    "    def process_func(x):\n",
    "        d = {}\n",
    "        \n",
    "        d['avg_month_lag'] = x['month_lag'].mean()\n",
    "        d['avg_installments'] = x['installments'].mean()\n",
    "        d['avg_days_between_purch'] = x['days_between_purch'].mean()\n",
    "        d['std_days_between_purch'] = x['days_between_purch'].std()\n",
    "        d['num_authorized'] = x['authorized_flag_binary'].sum()\n",
    "        d['avg_purchase_amount'] = x['purchase_amount'].mean()\n",
    "        d['std_purchase_amount'] = x['purchase_amount'].std()\n",
    "        d['min_purchase_amount'] = x['purchase_amount'].min()\n",
    "        d['max_purchase_amount'] = x['purchase_amount'].max()\n",
    "        d['num_purchases'] = x['purchase_amount'].count()\n",
    "        d['num_unique_merchants'] = x['merchant_id'].count()\n",
    "        d['avg_numerical_1'] = x['numerical_1'].mean()\n",
    "        d['std_numerical_1'] = x['numerical_1'].std()\n",
    "        d['min_numerical_1'] = x['numerical_1'].min()\n",
    "        d['max_numerical_1'] = x['numerical_1'].max() \n",
    "        d['avg_numerical_2'] = x['numerical_2'].mean()\n",
    "        d['std_numerical_2'] = x['numerical_2'].std()\n",
    "        d['min_numerical_2'] = x['numerical_2'].min()\n",
    "        d['max_numerical_2'] = x['numerical_2'].max()\n",
    "        d['avg_active_months_lag12'] = x['active_months_lag12'].mean()\n",
    "        d['avg_purchases_lag12'] = x['avg_purchases_lag12'].mean()\n",
    "\n",
    "        for col in vect_category_cols:\n",
    "            d[col] = x[col].sum()\n",
    "\n",
    "        return pd.Series(d, index=[\n",
    "            'avg_month_lag', 'avg_installments', 'avg_days_between_purch', 'std_days_between_purch',\n",
    "            'num_authorized', 'avg_purchase_amount', 'std_purchase_amount',\n",
    "            'min_purchase_amount', 'max_purchase_amount', 'num_purchases', 'avg_numerical_1', 'std_numerical_1',\n",
    "            'min_numerical_1', 'max_numerical_1', 'avg_numerical_2', 'std_numerical_2',\n",
    "            'min_numerical_2', 'max_numerical_2', 'avg_active_months_lag12', 'avg_purchases_lag12', 'num_unique_merchants'\n",
    "        ] + vect_category_cols)\n",
    "\n",
    "    # Since historical transactions can have more than one transaction made by \n",
    "    # the same card id, we group the dataset by card id and create summary \n",
    "    # statistics to describe transactions made by that card is.\n",
    "    final_df = hist_transactions.groupby(by='card_id').apply(process_func)\n",
    "\n",
    "    # Save the processed batch into a csv file.\n",
    "    print('Saving processed DF:', final_df.shape)\n",
    "    print(\"===============================\")\n",
    "    final_df.to_csv('df_new_' + str(i) + '.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the ideal number of batches to break dataset into.\n",
    "num_slices = int(hist_df.shape[0] / nrows_to_load) + 1\n",
    "\n",
    "# For each batch, run preprocessing function and save result to csv files.\n",
    "for i in range(num_slices):\n",
    "    process_batch(hist_df, merchants_df, i, final_set=True if (i+1)*nrows_to_load >= hist_df.shape[0] else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
