{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid', font_scale = 1.5, color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "n_rows_to_load = 1000000\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train + test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all csv files into the memory\n",
    "hist_transactions = pd.read_csv(\"/home/nikolaevra/datasets/elo/historical_transactions.csv\") #, index_col='card_id')\n",
    "# merchants = pd.read_csv(\"/home/nikolaevra/datasets/elo/merchants.csv\", index_col='merchant_id')\n",
    "# train_df = pd.read_csv(\"/home/nikolaevra/datasets/elo/train.csv\", index_col='card_id')\n",
    "test_df_orig = pd.read_csv(\"/home/nikolaevra/datasets/elo/test.csv\") #, index_col='card_id')\n",
    "\n",
    "# # First inner join historical transactions \n",
    "# # with train dataframe using the card_id column as index.\n",
    "# main_hist = hist_transactions.join(\n",
    "#     train_df,\n",
    "#     how='inner',\n",
    "#     rsuffix='_train',\n",
    "#     lsuffix='_transac'\n",
    "# ).drop_duplicates().reset_index(drop=True).set_index('merchant_id')\n",
    "\n",
    "# # Second inner join the first joined table with the merchants \n",
    "# # table using the merchant_id\n",
    "# main_df = main_hist.join(\n",
    "#     merchants,\n",
    "#     how='inner',\n",
    "#     rsuffix='_merchant'\n",
    "# ).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# # Shuffle the data.\n",
    "# main_df = main_df.sample(frac=1)\n",
    "\n",
    "# # Parse the datetime string into datetime object.\n",
    "# main_df['purchase_date'] = pd.to_datetime(main_df['purchase_date'])\n",
    "\n",
    "# # Parse datetime object and create dummy columns to store day, month and year count.\n",
    "# main_df['purchase_date_day'] = pd.to_datetime(\n",
    "#     main_df['purchase_date']\n",
    "# ).dt.day\n",
    "# main_df['purchase_date_month'] = pd.to_datetime(\n",
    "#     main_df['purchase_date']\n",
    "# ).dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop repeated columns\n",
    "drop_cols = [\n",
    "    'city_id_merchant', 'state_id_merchant', 'city_id',\n",
    "    'category_2_merchant', 'category_1_merchant', 'purchase_date',\n",
    "    'merchant_category_id_merchant', 'subsector_id_merchant', 'merchant_group_id',\n",
    "    'merchant_category_id', 'first_active_month', 'avg_sales_lag3', \n",
    "    'avg_purchases_lag3', 'active_months_lag3', 'avg_sales_lag6', \n",
    "    'avg_purchases_lag6', 'active_months_lag6','avg_sales_lag12', 'active_months_lag12'\n",
    "]\n",
    "\n",
    "main_df = main_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that have missing values.\n",
    "cols_missing_data = [main_df.columns[i] for i, missing_data in enumerate(main_df.isna().any()) if missing_data]\n",
    "\n",
    "for col in cols_missing_data:\n",
    "    main_df[col] = main_df[col].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all of the categorical columns so that we can \n",
    "# one hot encode them and later remove.\n",
    "categorical_cols = [\n",
    "    'category_1', 'category_2',\n",
    "    'category_3', 'category_4', \n",
    "    'authorized_flag', 'state_id', 'subsector_id',\n",
    "    'most_recent_sales_range', 'most_recent_purchases_range',\n",
    "    'purchase_date_day', 'purchase_date_month',\n",
    "    'feature_1', 'feature_2', 'feature_3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = [col for col in main_df.columns if col not in categorical_cols + ['target']]\n",
    "continuous_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = dict()\n",
    "\n",
    "for cat in categorical_cols:\n",
    "    unique_vals[cat] = list(main_df[cat].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_cols:\n",
    "    main_df[cat] = main_df[cat].astype('category', categories=unique_vals[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cat = categorical_cols[0]\n",
    "raw_cat = pd.get_dummies(main_df[first_cat]).values\n",
    "print('added feature:', first_cat, '->', raw_cat.shape)\n",
    "\n",
    "# For every categorical column, one hot encode it \n",
    "# and add to the main dataframe. \n",
    "for cat_col in categorical_cols[1:]:\n",
    "    dummies = pd.get_dummies(main_df[cat_col]).values\n",
    "    print(dummies.shape)\n",
    "    raw_cat = np.concatenate((raw_cat, dummies), axis = 1)\n",
    "    print('added feature:', cat_col, '->', raw_cat.shape)\n",
    "    \n",
    "# Drop all of the categorical columns because we have\n",
    "# replaced them with one hot encoded ones.     \n",
    "clean_oh_df = main_df.drop(columns = categorical_cols)\n",
    "\n",
    "# Convert target column into its own numpy array.\n",
    "raw_Y = clean_oh_df['target'].values\n",
    "\n",
    "# Remove target column, so that we can use the rest \n",
    "# as input columns.\n",
    "clean_oh_df = clean_oh_df.drop(columns=['target'])\n",
    "\n",
    "print('Non-categorical data:')\n",
    "clean_oh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x = clean_oh_df[continuous_cols[0]].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "raw_norm_cont = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "\n",
    "for col in continuous_cols[1:]:\n",
    "    x = clean_oh_df[col].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "    raw_norm_cont = np.concatenate((raw_norm_cont, x_scaled), axis=1)\n",
    "\n",
    "raw_X = np.concatenate((raw_cat, raw_norm_cont), axis=1)\n",
    "print(\"Final Shape:\", raw_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"X:\", raw_X.shape, \"Y:\", raw_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "split_point = int(split_ratio * raw_X.shape[0])\n",
    "\n",
    "# Split into train and test sets.\n",
    "print('Splitting at:', split_point)\n",
    "train_X = raw_X[:split_point, :]\n",
    "train_Y = raw_Y[:split_point]\n",
    "\n",
    "test_X = raw_X[split_point:, :]\n",
    "test_Y = raw_Y[split_point:]\n",
    "\n",
    "print(\"train_X\", train_X.shape)\n",
    "print(\"train_Y\", train_Y.shape)\n",
    "print(\"test_X\", test_X.shape)\n",
    "print(\"test_Y\", test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "import livelossplot\n",
    "plot_losses = livelossplot.PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(train_X.shape[1], )))\n",
    "# model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Train model\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=256,\n",
    "          epochs=30,\n",
    "          callbacks=[plot_losses],\n",
    "          verbose=1,\n",
    "          validation_data=(test_X, test_Y))\n",
    "\n",
    "# Score model\n",
    "score = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print('Test loss:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First inner join historical transactions \n",
    "# with train dataframe using the card_id column as index.\n",
    "test_hist = pd.merge(\n",
    "    test_df_orig, # left\n",
    "    hist_transactions, # right\n",
    "    how='left', \n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ")#.drop_duplicates().reset_index().set_index('merchant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second inner join the first joined table with the merchants \n",
    "# table using the merchant_id\n",
    "test_df = test_hist.join(\n",
    "    merchants,\n",
    "    how='right',\n",
    "    rsuffix='_merchant'\n",
    ").drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Parse the datetime string into datetime object.\n",
    "test_df['purchase_date'] = pd.to_datetime(test_df['purchase_date'])\n",
    "\n",
    "# Parse datetime object and create dummy columns to store day, month and year count.\n",
    "test_df['purchase_date_day'] = pd.to_datetime(\n",
    "    test_df['purchase_date']\n",
    ").dt.day\n",
    "test_df['purchase_date_month'] = pd.to_datetime(\n",
    "    test_df['purchase_date']\n",
    ").dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that have missing values.\n",
    "cols_missing_data = [test_df.columns[i] for i, missing_data in enumerate(test_df.isna().any()) if missing_data]\n",
    "\n",
    "for col in cols_missing_data:\n",
    "    test_df[col] = test_df[col].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categorical_cols:\n",
    "    test_df[cat] = test_df[cat].astype('category', categories=unique_vals[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cat = categorical_cols[0]\n",
    "raw_cat = pd.get_dummies(test_df[first_cat]).values\n",
    "print('added feature:', first_cat, '->', raw_cat.shape)\n",
    "\n",
    "# For every categorical column, one hot encode it \n",
    "# and add to the main dataframe. \n",
    "for cat_col in categorical_cols[1:]:\n",
    "    dummies = pd.get_dummies(test_df[cat_col]).values\n",
    "    print(dummies.shape)\n",
    "    raw_cat = np.concatenate((raw_cat, dummies), axis = 1)\n",
    "    print('added feature:', cat_col, '->', raw_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all of the categorical columns because we have\n",
    "# replaced them with one hot encoded ones.     \n",
    "clean_cnt_df = test_df.drop(columns=categorical_cols)\n",
    "\n",
    "# Convert target column into its own numpy array.\n",
    "card_ids = clean_cnt_df['card_id'].values\n",
    "\n",
    "# Remove target column, so that we can use the rest \n",
    "# as input columns.\n",
    "clean_cnt_df = clean_cnt_df.drop(columns=['card_id'])\n",
    "\n",
    "print('Non-categorical data:')\n",
    "clean_cnt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x = clean_cnt_df[continuous_cols[0]].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "raw_norm_cont = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "\n",
    "for col in continuous_cols[1:]:\n",
    "    x = clean_cnt_df[col].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "    raw_norm_cont = np.concatenate((raw_norm_cont, x_scaled), axis=1)\n",
    "\n",
    "raw_X = np.concatenate((raw_cat, raw_norm_cont), axis=1)\n",
    "print(\"Final Shape:\", raw_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(raw_X)\n",
    "answer = np.concatenate((card_ids, predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame.from_records(data, columns=['card_id', 'target'])\n",
    "df.to_csv(\"predictions.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n",
    "        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "df_a = pd.DataFrame(raw_data, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {\n",
    "        'subject_id': ['4', '5', '6', '7', '8'],\n",
    "        'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], \n",
    "        'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan']}\n",
    "df_b = pd.DataFrame(raw_data, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_orig.isna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_orig = test_df_orig.reset_index()\n",
    "hist_transactions = hist_transactions.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df_orig['card_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df_orig = test_df_orig.set_index('card_id')\n",
    "hist_transactions = hist_transactions.set_index('card_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = hist_transactions.join(test_df_orig, how='left')\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df_orig.reset_index()['card_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(df_new.reset_index()['card_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
